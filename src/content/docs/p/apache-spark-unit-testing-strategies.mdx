---
title: "Apache Spark Unit Testing Strategies"
description: "#scala #programming #apachespark #bigdata"
publishedDate: 2025-12-05
author: "Sukumaar"
image: "/images/blog/apache-spark-unit-testing-strategies-01.jpg"
tags: ["tutorial","bigdata-cloud"]
draft: false
---
> Recipe/Guide about writing unit tests for Apache Spark with Scala (mainly for beginners).

**Recipe complexity level:** â—¼ï¸â—»ï¸â—»ï¸â—»ï¸

**Recipe prerequisite:**
- Some knowledge of Big Data, Apache Spark, Scala, Java.

**Recipe ingredients:**
- Your favorite IDE : Intellij or VSCode (with Metals)
- sbt / maven installed (sbt is used in this tutorial)
- jdk 8
- scala 2.12 ğŸ˜

### Unit testing ?
In computer programming, unit testing is a software testing method by which individual units of source codeâ€”sets of one or more computer program modules together with associated control data, usage procedures, and operating proceduresâ€”are tested to determine whether they are fit for use Wikipedia

Writing unit tests of the code before writing the actual code is a brilliant strategy used in TDD.

### TDD ?
Test-driven development (TDD) is a software development process relying on software requirements being converted to test cases before software is fully developed, and tracking all software development by repeatedly testing the software against all test cases. Wikipedia

I will skip writing about the advantages of writing unit tests or the advantages of TDD (because there are so many that I need to write a separate article for it.)

This code sample uses (super awesome) Scalatest ğŸ˜ğŸ¤© testing framework.

### ScalaTest:

- It is the most flexible and most popular testing tool in the Scala ecosystem.link
- With so many other features, it allows designing tests with multiple styles.
    - ScalaTest supports different styles of testing, each designed to address a particular set of needs. link
    - There are separate traits for these styles.
    - JUnit lovers can use the `AnyFunSuite` trait.

___
Project Creation:
Directory structure of my project:
```bash
.
â”œâ”€â”€ build.sbt
â””â”€â”€ src
    â”œâ”€â”€ main
    â”‚   â””â”€â”€ scala
    â”‚       â””â”€â”€ sukumaar
    â”‚           â””â”€â”€ App.scala #This doesn't have any imp code
    â””â”€â”€ test
        â””â”€â”€ scala
            â””â”€â”€ sukumaar
                â”œâ”€â”€ AppTest.scala
                â””â”€â”€ TraitSparkSessionTest.scala
```

`build.sbt` I used :
```scala
name := "sample-spark-scala-project"
version := "1.0"
scalaVersion := "2.12.13"

val sparkVersion = "2.4.0"

libraryDependencies += 
    "org.apache.spark" %% "spark-core" % sparkVersion
libraryDependencies += 
    "org.apache.spark" %% "spark-sql" % sparkVersion
libraryDependencies += 
    "org.scalatest" %% "scalatest" % "3.2.9" % Test

/*
// you can always use this dependency if you are 
// going to use only funsuite
libraryDependencies += 
    "org.scalatest" %% "scalatest-funsuite" % "3.2.11" % "test"
*/
```
Import this project to your favorite IDE.
If you prefer CLI (like a mature developer ğŸ˜›) then use this command:
`sbt clean compile`